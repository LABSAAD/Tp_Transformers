{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1e39861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline,set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cacb57",
   "metadata": {},
   "source": [
    "# sentiment analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ae06abd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French sentence:\n",
      "\n",
      "[{'label': 'POSITIVE', 'score': 0.5274483561515808}]\n",
      "English sentences:\n",
      "\n",
      "Label: POSITIVE\n",
      "Confidence Score: 69.22613978385925\n",
      "Label: NEGATIVE\n",
      "Confidence Score: 98.81595969200134 \n",
      "\n",
      "Arabic sentences:\n",
      "\n",
      "Label: NEGATIVE\n",
      "Confidence Score: 62.5647246837616\n",
      "Label: POSITIVE\n",
      "Confidence Score: 65.83120226860046\n"
     ]
    }
   ],
   "source": [
    "#DistilBERT model\n",
    "token= pipeline('sentiment-analysis')\n",
    "#french sentence\n",
    "print(\"French sentence:\\n\")\n",
    "sen=\"J'aime dormir tard tous les jours.\"\n",
    "res = token(sen)\n",
    "print(res)\n",
    "#print(\"Label:\", res['label'])\n",
    "#print(\"Confidence Score:\", res['score']*100 ,\"\\n\")\n",
    "\n",
    "#English sentences\n",
    "print(\"English sentences:\\n\")\n",
    "pos_text = \"I like sleeping late everyday.\"\n",
    "res = token(pos_text)[0]\n",
    "print(\"Label:\", res['label'])\n",
    "print(\"Confidence Score:\", res['score']*100)\n",
    "    \n",
    "neg_text = \"I dislike sleeping late everyday.\"\n",
    "res = token(neg_text)[0]\n",
    "print(\"Label:\", res['label'])\n",
    "print(\"Confidence Score:\", res['score']*100,\"\\n\")\n",
    "  \n",
    "#arabic sentences\n",
    "print(\"Arabic sentences:\\n\")\n",
    "phrase = \"أحب أن أنام متأخرًا كل يوم\"\n",
    "phrase1=\"سعدت بلقائك\"\n",
    "\n",
    "\n",
    "res = token(phrase)[0]\n",
    "print(\"Label:\", res['label'])\n",
    "print(\"Confidence Score:\", res['score']*100)\n",
    "\n",
    "res = token(phrase1)[0]\n",
    "print(\"Label:\", res['label'])\n",
    "print(\"Confidence Score:\", res['score']*100)\n",
    "\n",
    "#remarque : la phrase en anglais nous donne Positive tandis que sa traduction mot à mot nous donne Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682c6f38",
   "metadata": {},
   "source": [
    "# Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d63561d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/labrhaddasaad/opt/anaconda3/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:843: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': 'Hello everyone. I\\'ll let you know if I do.\\n\\nWith a little polish on my part, here\\'s a post about some of the awesome things that are happening on the ground so far. The \"Lambda Literary Agency\" at'}\n",
      "\n",
      "{'generated_text': 'Hello everyone.\\n\\nThe team, all from Japan and the United States, have sent a special team to Syria. When we go out of the way to say this, thank you to everybody who was there. We\\'ve been working very well,\"'}\n",
      "\n",
      "{'generated_text': \"Hello everyone, and let, you know that the last week to get on the mailing list about V1.4 comes late. It wasn't all bad. The initial release got a lot of bugs fixed from 3.2 to V1.2\"}\n",
      "\n",
      "{'generated_text': \"Hello everyone, the most important thing at launch is really getting you from this point forward. We're doing a lot better this week. That was the big challenge of the Kickstarter. We've got a really great team right now. Some of the guys\"}\n",
      "\n",
      "{'generated_text': 'Hello everyone,\\n\\nThe ECCA and ERCMP are working extremely hard to bring some great news to the people of Canada, and while it is still early on in our investigations, we have a number of things we believe have to happen'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "#Anglais\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"gpt2\")\n",
    "res = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "res= res(\"Hello everyone\", max_length=50, num_return_sequences=5)\n",
    "for i in range(0,5):\n",
    "        print(res[i])\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8ddaf53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': 'Bonjour । Le mot \" karma \" est un mot qui signifie \" sagesse \" . Le , il est nommé ministre de l\\' Intérieur dans le gouvernement de coalition de coalition de la coalition de centre-droit de la coalition de centre-'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#français\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"antoiloui/belgpt2\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"antoiloui/belgpt2\")\n",
    "res = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "res= res(\"Bonjour \", max_length=50, num_return_sequences=1)[0]\n",
    "print(res)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a750d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اللغة العربية،والسرد،ولأسلوبه لا يثير بالملل\"\n",
      "\"كتاب خفيف . بس عجبني فيه بعض المشاكل التي تحدث عنها في الكتاب غير قليل\"\n",
      "\"الكتاب يحتوي على مواضيع مختلفة عن المرأة اليومية مع الناس .و بعضها ممتع\"\n",
      "\"كان\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اللغة العربية، حيث كانت الرواية مملة بعض الشيء. ولم تضف لي شيئاً من البحث عن الأسماء في السعودية من أحداث عدة مراحل، ولكن الرواية الأولى لها مبالغة، حيث أني تمنيت لو أنها كانت عن علاقة غير دقيق لها. أما قصة الحب والمرأة التي لا\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اللغة العربية التي تدور مع أبطالها، خاصة في تلك الأيام كان بها أي عمل. وقد بدأ الكاتب في بعض الأحيان أنه أراد أن يترك شيئاً جديداً في صورة واقعية. حيث أن معظم الناس لا يملكون هذا الكم من الألم و الأشخاص هم أيضاً كل\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اللغة العربية فيها بعض الشيء الكثير من الكلام الغير متكلف في السرد ، إضافة إلى ذلك أكثر من التكرار في الوصف وعدم وجود الفكرة الأدبية ولكن هذا لا توجد شيءٌ من التفصيل ! .\"\n",
      "\"كنت أعتقد بأن غازي كان من ناحية الأسلوب و لا يختلف كثيرا عن\n",
      "\n",
      "اللغة العربية.لكن لم ترق لي لكن شعرت أن رواية \"\"ذاكرة الجسد\"\" ذات علاقة . كانت من الممكن ان تكون سيئة بالنسبة لي\"\n",
      "\"تُعجبني القصة . اسلوب الكاتبة حلو و فكرة غير واقعي\"\n",
      "\"لغة بسيطة . جميلة ، كانت\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#arabe\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mofawzy/gpt2-arabic-sentence-generator\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"mofawzy/gpt2-arabic-sentence-generator\")\n",
    "\n",
    "for i in range(0,5):\n",
    "    res = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    test= res(\"اللغة العربية\", max_length=50, do_sample=True)[0]\n",
    "    print(test[\"generated_text\"])\n",
    "    print()\n",
    "    i=i+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dd0772",
   "metadata": {},
   "source": [
    "# Name entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "678fc772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-MISC', 'score': 0.9980936, 'index': 1, 'word': 'European', 'start': 0, 'end': 8}, {'entity': 'I-ORG', 'score': 0.9990613, 'index': 4, 'word': 'Google', 'start': 27, 'end': 33}]\n"
     ]
    }
   ],
   "source": [
    "token = pipeline(\"ner\")\n",
    "\n",
    "txt = \"European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices\"\n",
    "    \n",
    "print(token(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b93bd8c",
   "metadata": {},
   "source": [
    "# Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "6718a65d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9864248037338257, 'start': 21, 'end': 27, 'answer': 'tennis'}\n",
      "{'score': 0.05860627442598343, 'start': 108, 'end': 122, 'answer': 'He is an angel'}\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"question-answering\")\n",
    "\n",
    "context = r\"\"\"\n",
    "Robin loves playing tennis, because his father was a tennis player. He is very strong, and his father too. He is an angel: he helps his Mummy, he does his homework, he does the housework. He is keen on his parents. He is at the top of the class, so he's very bright !\n",
    "He never chats in class, he always listens to the teacher, he often asks questions.\n",
    "\"\"\"\n",
    "\n",
    "print(nlp(question=\"What does Robin love playing ?\", context=context))\n",
    "print(nlp(question=\"Is Robin a naughty child ?\", context=context))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160c0de0",
   "metadata": {},
   "source": [
    "# Filling masked text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ca933bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'google est une.',\n",
       "  'score': 0.15166300535202026,\n",
       "  'token': 1012,\n",
       "  'token_str': '.'},\n",
       " {'sequence': 'google est une!',\n",
       "  'score': 0.08290103822946548,\n",
       "  'token': 999,\n",
       "  'token_str': '!'},\n",
       " {'sequence': 'google est une?',\n",
       "  'score': 0.08247017115354538,\n",
       "  'token': 1029,\n",
       "  'token_str': '?'},\n",
       " {'sequence': 'google est une vie',\n",
       "  'score': 0.046329569071531296,\n",
       "  'token': 20098,\n",
       "  'token_str': 'vie'},\n",
       " {'sequence': 'google est une ;',\n",
       "  'score': 0.019203977659344673,\n",
       "  'token': 1025,\n",
       "  'token_str': ';'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline('fill-mask', model='bert-large-uncased-whole-word-masking')\n",
    "unmasker(\"Rabat is the [MASK] of Morocco.\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931b858c",
   "metadata": {},
   "source": [
    "# Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8f983a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' Man went from victim to aggressor in matter of minutes after firefighters intervened in Toulouse . The 36-year-old man broke the bay window of his apartment by breaking it . He insulted and threatened the firefighters who treated him before police arrived .'}]\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "arti=\"\"\" He went from victim to aggressor in a matter of minutes. Tuesday evening, the firefighters intervened \n",
    "on avenue Jean-Rieux, in Toulouse , after a man was injured by breaking the bay window of his apartment.\n",
    "When they got there, they quickly realized that the victim was not in his normal state.\n",
    "After administering first aid to his leg, which was bleeding, rescuers noticed that the patient was heating up.\n",
    "Until giving a head butt to one of the firefighters who had just treated him.The latter then decided to leave \n",
    "the apartment while awaiting the arrival of the police. But the 36-year-old didn't stop there: \n",
    "he insulted and threatened them on numerous occasions. Until the arrival of the police who finally managed \n",
    "to subdue him. The man was taken to hospital, before being arrested. He was still in custody on Thursday.\n",
    "The firefighters have filed a complaint.\"\"\"\n",
    "summary = summarizer(arti, max_length=130, min_length=30)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c93fb9",
   "metadata": {},
   "source": [
    "# Translation with Google's T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3131beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Je crois pouvoir voler, je crois pouvoir toucher le ciel'}]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"t5-small\")\n",
    "res = pipeline(\"translation_en_to_fr\", model=model, tokenizer=tokenizer)\n",
    "print(res(\"I believe I can fly, I believe I can touch the sky\", max_length=40))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a7e92e",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d43fa381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.574217   -0.47503227 -0.00415378 ... -0.45464733 -0.03436462\n",
      "  -0.9342763 ]\n",
      " [-0.2833405  -0.48915017  0.42437726 ... -0.1363443  -0.10375428\n",
      "  -1.1598346 ]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"I believe I can fly\", \"I believe I can touch the sky\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda6b09c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
